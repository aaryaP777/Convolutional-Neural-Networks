{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhSl6M7F14UB/DWZtLfsDe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aaryaP777/Convolutional-Neural-Networks/blob/main/Tensorflow_text_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mG29igw4m5K2"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization"
      ],
      "metadata": {
        "id": "oWn5yONynFJr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"I am a hero.\",\n",
        "    \"I like burgers and fries.\",\n",
        "    \"I love movies and games.\",\n",
        "    \"I hate monsoon.\"\n",
        "]"
      ],
      "metadata": {
        "id": "bKSSQIIfnPJP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# maximum vocabulary size\n",
        "\n",
        "MAX_VOCAB_SIZE = 20_000"
      ],
      "metadata": {
        "id": "ywbxB1bEnje1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization_layer = TextVectorization(max_tokens=MAX_VOCAB_SIZE)"
      ],
      "metadata": {
        "id": "ozll-FH4oOdP"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization_layer.adapt(sentences)"
      ],
      "metadata": {
        "id": "S8HUAK3OoT3L"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = vectorization_layer(sentences)\n",
        "print(sequences)\n",
        "\n",
        "# each word of each sentence is given an integer value.\n",
        "# matrix size = size of longest sentence."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAjwEd0noW_w",
        "outputId": "f5e4a334-c933-4068-9ae6-2f3b2fe5d573"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 2 13 14  8  0]\n",
            " [ 2  7 12  3 11]\n",
            " [ 2  6  4  3 10]\n",
            " [ 2  9  5  0  0]], shape=(4, 5), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorization_layer.get_vocabulary()\n",
        "\n",
        "# get the list of words accroding to integer values."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dm016dfogyf",
        "outputId": "506d31e5-ff10-498a-a1f3-42b7e4f7c57e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " np.str_('i'),\n",
              " np.str_('and'),\n",
              " np.str_('movies'),\n",
              " np.str_('monsoon'),\n",
              " np.str_('love'),\n",
              " np.str_('like'),\n",
              " np.str_('hero'),\n",
              " np.str_('hate'),\n",
              " np.str_('games'),\n",
              " np.str_('fries'),\n",
              " np.str_('burgers'),\n",
              " np.str_('am'),\n",
              " np.str_('a')]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# word to index mappping\n",
        "\n",
        "word_index = {v:k for k, v in enumerate(vectorization_layer.get_vocabulary())}\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UShqPaBpX0A",
        "outputId": "831c2fd8-f34a-4065-94d5-ce65b79df9f1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': 0, '[UNK]': 1, np.str_('i'): 2, np.str_('and'): 3, np.str_('movies'): 4, np.str_('monsoon'): 5, np.str_('love'): 6, np.str_('like'): 7, np.str_('hero'): 8, np.str_('hate'): 9, np.str_('games'): 10, np.str_('fries'): 11, np.str_('burgers'): 12, np.str_('am'): 13, np.str_('a'): 14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# truncation\n",
        "vectorization_layer_truncated = TextVectorization(\n",
        "    max_tokens = MAX_VOCAB_SIZE,\n",
        "    output_sequence_length = 3\n",
        ")\n",
        "\n",
        "# fit\n",
        "vectorization_layer_truncated.adapt(sentences)\n",
        "\n",
        "# vectorize\n",
        "sequences_truncated = vectorization_layer_truncated(sentences)\n",
        "print(sequences_truncated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2IfV7QRrzx9",
        "outputId": "50b20d6c-0fff-4e34-a3ca-54c30d845d82"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 2 13 14]\n",
            " [ 2  7 12]\n",
            " [ 2  6  4]\n",
            " [ 2  9  5]], shape=(4, 3), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ragged (outputs can have different lengths)\n",
        "vectorization_layer_ragged = TextVectorization(\n",
        "    max_tokens = MAX_VOCAB_SIZE,\n",
        "    ragged = True,\n",
        ")\n",
        "\n",
        "# fit\n",
        "vectorization_layer_ragged.adapt(sentences)\n",
        "\n",
        "# vectorize\n",
        "sequences_ragged = vectorization_layer_ragged(sentences)\n",
        "print(sequences_ragged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyESVUk8rxcx",
        "outputId": "578073af-fe7e-4258-f4ae-b902fbd31146"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[2, 13, 14, 8], [2, 7, 12, 3, 11], [2, 6, 4, 3, 10], [2, 9, 5]]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# front padding\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "\n",
        "padded = pad_sequences(sequences_ragged.to_list())\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GufCBjRuvgLP",
        "outputId": "f60ba1f5-4872-48c1-8ac1-1f43b452ddfe"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  2 13 14  8]\n",
            " [ 2  7 12  3 11]\n",
            " [ 2  6  4  3 10]\n",
            " [ 0  0  2  9  5]]\n"
          ]
        }
      ]
    }
  ]
}